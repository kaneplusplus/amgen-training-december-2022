---
title: "Amgen R Training Part 1: Data Manipulation"
format: html
---

# What are we going to do?

## Reading and summarizing the Social Vulnerability Index

- Importing the data and the data dictionary.
- The new(ish) pipe operator.
- Tidy vs base - what's the difference?
- Summarize the data using `dplyr`, `purrr`, `DT`, `gtsummary`, and `naniar`.
- Spatial visualizations

# [The CDC Social Vulnerability Index](https://www.atsdr.cdc.gov/placeandhealth/svi/index.html)

## What is it?

- Social vulnerability ["refers to the potential negative effects on communities caused by external stresses on human health. Such stresses include natural or human-caused disasters, or disease outbreaks. Reducing social vulnerability can decrease both human suffering and economic loss."](https://www.atsdr.cdc.gov/placeandhealth/svi/index.html)
- Data derived from ["15 U.S. census variables to help local officials identify communities that may need support before, during, or after disasters."](https://www.atsdr.cdc.gov/placeandhealth/svi/index.html)

## Where do I get it?

- [https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html](https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html)

## Which files are included?

- A `2018-data` directory.
- `SVI_2018.zip`
- `data-dictionary-2018.csv` - the data dictionary.
- `SVI2018Documentation_01192022_1.pdf` - written documentation.

# Importing the data and the data dictionary

## Symbols vs. values

```{r message = FALSE, warning=FALSE}
library(readr)
library(DT)
library(dplyr)
library(sf)
sf_use_s2(FALSE)

dd <- read_csv(file.path("2018-data", "data-dictionary-2018.csv"))

dd[, c("var_name", "description")]

select(dd, var_name, description)
```

## Function composition with pipes

```{r}
# %>% or |>
dd |> select(var_name, description)

quote(dd |> select(var_name, description))

dds = select(dd, var_name, description)
datatable(dds)
```

## Making tables interactive.

```{r}
dd |> 
  select(var_name, description) |>
  datatable()
```

## `attributes()` and types.

```{r}
library(sf)
d <- read_sf(file.path("2018-data", "SVI2018_US_tract.shp"))
d
class(d)
```

# Cleaning

# Functional vs. procedural programming

```{r}
library(purrr)

# Changes the -999 (missing) to NA's.

# Using the column index.
db <- d

for (j in seq_len(ncol(db))) {
  if (is.double(db[[j]])) {
    db[[j]][ db[[j]] == -999 ] <- NA
  }
}

# Using the column names.
db <- d

dbl_col_names = names(which(map_lgl(db, is.double)))

for (dcn in dbl_col_names) {
  db[ db[[dcn]] == -999 ,dcn] <- NA
}

# or using functions
mnnn_to_na <- function(x) {
  x[x == -999] <- NA
  x
}

db <- mutate_if(d, is.double, mnnn_to_na)

d <- mutate_if(d, is.double, ~ if_else(.x == -999, NA_real_, .x))

```

# Exploratory visualizations

## Missingness

```{r}
library(naniar)
library(DT)

# Create a table of the number and percent missing by variable.
d |>
  miss_var_summary() |>
  mutate(`Percent Missing` = round(100 * pct_miss, 2)) |>
  rename(`Number Missing` = n_miss, Variable = variable) |>
  select(-pct_miss) |>
  datatable()

gg_miss_upset(d)
```

# Table summaries

## The data dictionary

```{r}
datatable(dd)
```

## Summarizing 

```{r}
library(gtsummary)

# Create a table of SVI variables.
d |> 
  filter(STATE == "CALIFORNIA") |>
  as_tibble() |>
  select(AREA_SQMI, E_UNEMP, E_POV, E_PCI) |>
  tbl_summary(missing_text = "NA")
```

## Conditional summaries

```{r}
# Create a table of SVI variables by county.
d |> 
  filter(STATE == "CALIFORNIA") |>
  as_tibble() |>
  select(AREA_SQMI, E_UNEMP, E_POV, E_PCI, COUNTY) |>
  tbl_summary(by = "COUNTY", missing_text = "NA")
```

## Which counties in California have the biggest difference in census tract GDPs?

```{r}
# Calculate disparity by county in California.
ctd = d |>
  filter(STATE == "CALIFORNIA") |>
  group_by(COUNTY) |>
  summarize(disparity = max(E_PCI, na.rm = TRUE) / min(E_PCI, na.rm = TRUE)) |>
  suppressMessages()
ctd
```

# How do we visualize this?

```{r}
library(ggplot2)

p1 = ggplot(ctd, aes(geometry = geometry, fill = log10(disparity))) +
  geom_sf() +
  theme_bw()

p1
```

## What if we want better contrast between counties?

```{r}
library(viridis) # for scale_fill_viridis()
library(patchwork)

p2 = ggplot(ctd, aes(geometry = geometry, fill = log10(disparity))) +
  geom_sf() +
  scale_fill_viridis() +
  theme_bw()

p1 + p2
```

## Is this a "fair" measure?

```{r}

# Calculate disparity by state and county.
# Note: county names may repeat across states.
disp = d |>
  as_tibble() |>
  group_by(STATE, COUNTY) |>
  summarize(
    disparity = max(E_PCI, na.rm = TRUE) / min(E_PCI, na.rm = TRUE),
    num_fips = length(E_PCI),
    .groups = "drop") |>
  filter(is.finite(disparity)) |>
  arrange(desc(disparity)) |>
  mutate(state_county = paste(STATE, COUNTY)) |>
  mutate(state_county = factor(state_county, levels = state_county))

ggplot(disp, aes(x = num_fips, y = disparity)) +
  scale_y_log10() +
  scale_x_log10() +
  geom_point() +
  ylab("Disparity") +
  xlab("County Size (in Number of FIPS Areas)") +
  theme_bw()
```

# Unsolicited advice

1. Someday you may have to read your code - write with this in mind.
1. A pipeline should do 1 thing - you should be able to explain what this is.
1. If you reuse a pipeline wrap it in a function and document it otherwise comment it.
1. Make variable names as specific and short as possible but prioritize specific and use context.